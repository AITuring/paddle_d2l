{"nbformat":4,"nbformat_minor":2,"metadata":{"colab":{"name":"chap3.2.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true,"authorship_tag":"ABX9TyNppgY9jBvNdh5Am8VfAzeW"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"source":["!pip install -U d2l\n","!python -m pip install paddlepaddle -i https://mirror.baidu.com/pypi/simple"],"outputs":[],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"y1gSSXSXnlLA","executionInfo":{"status":"ok","timestamp":1630467962902,"user_tz":-480,"elapsed":31755,"user":{"displayName":"hello world","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjXbsfhC6hSDTQmFFNhXTddUBf1Yxa-RGB3KxXxFEA=s64","userId":"03112899976397097128"}},"outputId":"f4fc6c9e-9a14-444f-820e-f449d28cac0c"}},{"cell_type":"markdown","source":["# 3.2 线性回归从0开始实现"],"metadata":{"id":"9fj_G20TsPEk"}},{"cell_type":"code","execution_count":null,"source":["import random\n","# torch\n","import torch\n","from d2l import torch as d2l\n","# paddle\n","import numpy as np\n","import paddle\n","import matplotlib.pyplot as plt"],"outputs":[],"metadata":{"id":"Rww47zn8oH4Y","executionInfo":{"status":"ok","timestamp":1630468020425,"user_tz":-480,"elapsed":6806,"user":{"displayName":"hello world","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjXbsfhC6hSDTQmFFNhXTddUBf1Yxa-RGB3KxXxFEA=s64","userId":"03112899976397097128"}}}},{"cell_type":"markdown","source":["## 3.2.1生成数据集\n","\n","这里根据带有噪声的线性模型构造一个人造数据集。然后使用这个有限样本的数据集恢复此模型的参数。为了方便可视化，这里采用了低维数据。\n","\n","下面代码中，生成了一个包含1000个样本的数据集，每个样本包含从标准正态分布中采样的两个特征。合成数据集是一个矩阵：\n","\n","$$X\\in R^{100 \\times 2}$$\n","\n","使用线性模型参数$w=[2,-3.4]^T$、$b=4.2$和噪声项$\\varepsilon$生成数据集及其标签：\n","$$y=Xw+b+\\varepsilon$$\n","\n","你可以将$\\varepsilon$视为捕获特征和标签时的潜在观测误差。在这里我们认为标准假设成立，即$\\varepsilon$服从均值为0的正态分布。 为了简化问题，我们将标准差设为0.01。下面的代码生成合成数据集。"],"metadata":{"id":"lzwAQI8opRTR"}},{"cell_type":"markdown","source":["#### torch版"],"metadata":{"id":"jS92Ymwuo9OB"}},{"cell_type":"code","execution_count":null,"source":["def synthetic_data(w, b, num_examples):\n","    \"\"\"生成 y = Xw + b + 噪声。\"\"\"\n","    X = torch.normal(0, 1, (num_examples, len(w)))\n","    print(X.shape)\n","    y = torch.matmul(X, w) + b\n","    y += torch.normal(0, 0.01, y.shape)\n","    return X, y.reshape((-1, 1))\n","\n","true_w = torch.tensor([2, -3.4])\n","true_b = 4.2\n","features, labels = synthetic_data(true_w, true_b, 1000)\n","print('features:', features[0], '\\nlabel:', labels[0])"],"outputs":[],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"mGElzcErodc0","executionInfo":{"status":"ok","timestamp":1630468727017,"user_tz":-480,"elapsed":549,"user":{"displayName":"hello world","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjXbsfhC6hSDTQmFFNhXTddUBf1Yxa-RGB3KxXxFEA=s64","userId":"03112899976397097128"}},"outputId":"f151b765-4ebd-47fa-bdf2-d1a7093dc52b"}},{"cell_type":"code","execution_count":null,"source":["d2l.set_figsize()\n","d2l.plt.scatter(features[:, (1)].detach().numpy(),\n","                labels.detach().numpy(), 1);"],"outputs":[],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":244},"id":"P91kkaZ2pKkv","executionInfo":{"status":"ok","timestamp":1630468731103,"user_tz":-480,"elapsed":486,"user":{"displayName":"hello world","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjXbsfhC6hSDTQmFFNhXTddUBf1Yxa-RGB3KxXxFEA=s64","userId":"03112899976397097128"}},"outputId":"b61a03a8-ec4a-4288-fe08-8ddd06ae80bf"}},{"cell_type":"markdown","source":["#### d2l版"],"metadata":{"id":"bljr6ucCeI74"}},{"cell_type":"code","execution_count":null,"source":["def synthetic_data(w, b, num_examples):\n","    \"\"\"生成 y = Xw + b + 噪声。\"\"\"\n","    X = np.random.normal(0, 1, (num_examples, len(w)))\n","    print(X.shape, 'X')\n","    y = np.dot(X, w) + b\n","    print(y.shape, 'y')\n","    y += np.random.normal(0, 0.01, y.shape)\n","    return X, y.reshape((-1, 1))\n","\n","d2l_w = np.array([2, -3.4])\n","print(d2l_w.shape, 'd2l_w')\n","d2l_b = 4.2\n","d2l_features, d2l_labels = synthetic_data(d2l_w, d2l_b, 1000)"],"outputs":[],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"n2Im8SYEeXHL","executionInfo":{"status":"ok","timestamp":1630468531981,"user_tz":-480,"elapsed":399,"user":{"displayName":"hello world","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjXbsfhC6hSDTQmFFNhXTddUBf1Yxa-RGB3KxXxFEA=s64","userId":"03112899976397097128"}},"outputId":"e16e7809-bf43-44c8-edbf-580b9c1a3bf8"}},{"cell_type":"code","execution_count":null,"source":["plt.scatter(d2l_features[:, (1)], d2l_labels, 1);"],"outputs":[],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":265},"id":"YO80bqtUfZL_","executionInfo":{"status":"ok","timestamp":1630468704025,"user_tz":-480,"elapsed":864,"user":{"displayName":"hello world","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjXbsfhC6hSDTQmFFNhXTddUBf1Yxa-RGB3KxXxFEA=s64","userId":"03112899976397097128"}},"outputId":"f7023816-8b93-41ae-a670-7708041fdafe"}},{"cell_type":"markdown","source":["#### paddle版"],"metadata":{"id":"de7r1_nOsBau"}},{"cell_type":"code","execution_count":null,"source":["paddle.utils.run_check()"],"outputs":[],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"uJlA2ebZyCUz","executionInfo":{"status":"ok","timestamp":1630468319146,"user_tz":-480,"elapsed":544,"user":{"displayName":"hello world","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjXbsfhC6hSDTQmFFNhXTddUBf1Yxa-RGB3KxXxFEA=s64","userId":"03112899976397097128"}},"outputId":"71a17af8-135f-4e94-f5a1-2061688463fb"}},{"cell_type":"code","execution_count":null,"source":["def synthetic_data_paddle(w, b, num_examples):\n","  \"\"\"Generate y = Xw + b + noise.\"\"\"\n","  X = np.random.normal(0, 1, (num_examples, len(w)))\n","  y = np.dot(X, w) + b\n","  y += np.random.normal(0, 0.01, y.shape)\n","  return X, y.reshape((-1, 1))\n","\n","paddle_w = np.array([2, -3.4])\n","# paddle_w = paddle.to_tensor([2, -3.4])\n","paddle_b = 4.2\n","paddle_features, paddle_labels = synthetic_data_paddle(paddle_w, paddle_b, 1000)\n","print('paddle features:', paddle_features[0], '\\nlabel:', paddle_labels[0])"],"outputs":[],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"oPHesS0XsafQ","executionInfo":{"status":"ok","timestamp":1630468367025,"user_tz":-480,"elapsed":541,"user":{"displayName":"hello world","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjXbsfhC6hSDTQmFFNhXTddUBf1Yxa-RGB3KxXxFEA=s64","userId":"03112899976397097128"}},"outputId":"edd97531-1b75-46ea-8567-655e304a7f94"}},{"cell_type":"code","execution_count":null,"source":["plt.scatter(paddle_features[:, (1)], paddle_labels, 1);"],"outputs":[],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":265},"id":"JEiHaF-15fwU","executionInfo":{"status":"ok","timestamp":1630468371932,"user_tz":-480,"elapsed":471,"user":{"displayName":"hello world","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjXbsfhC6hSDTQmFFNhXTddUBf1Yxa-RGB3KxXxFEA=s64","userId":"03112899976397097128"}},"outputId":"f9b41e4b-8489-40ed-f4ac-81f40334ed39"}},{"cell_type":"markdown","source":["## 3.2.2 读取数据集\n","\n","训练模型时要对数据遍历，每次抽取一小批量样本，并使用它们来更新模型。由于此过程是训练机器学习算法的基础，所以有必要定义一个函数，该函数能够打乱数据集中的样本并以小批量的方式获取数据。\n","\n","在下面代码中，定义了一个`data_iter`函数，该函数接收批量大小、特征矩阵和标签向量作为输入，生成大小为`batch_size`的小批量，每个小批量包含一组特征和标签。"],"metadata":{"id":"EkWtZ1hYfz_1"}},{"cell_type":"markdown","source":["#### torch版"],"metadata":{"id":"A2i6VrTccouQ"}},{"cell_type":"code","execution_count":null,"source":["def data_iter(batch_size, features, labels):\n","    print(batch_size)\n","    print(features.shape)\n","    print(labels.shape)\n","    num_examples = len(features)\n","    print(num_examples)\n","    indices = list(range(num_examples))\n","    # 这些样本是随机读取的，没有特定的顺序\n","    random.shuffle(indices)\n","    for i in range(0, num_examples, batch_size):\n","        batch_indices = torch.tensor(indices[i:min(i +\n","                                                   batch_size, num_examples)])\n","        yield features[batch_indices], labels[batch_indices]"],"outputs":[],"metadata":{"id":"adO4c90ef3wH","executionInfo":{"status":"ok","timestamp":1630468785159,"user_tz":-480,"elapsed":518,"user":{"displayName":"hello world","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjXbsfhC6hSDTQmFFNhXTddUBf1Yxa-RGB3KxXxFEA=s64","userId":"03112899976397097128"}}}},{"cell_type":"markdown","source":["#### paddle版"],"metadata":{"id":"v_z2vKjncroB"}},{"cell_type":"code","execution_count":null,"source":["def paddle_data_iter(batch_size, features, labels):\n","    num_examples = len(features)\n","    indices = list(range(num_examples))\n","    # 这些样本是随机读取的，没有特定的顺序\n","    random.shuffle(indices)\n","    for i in range(0, num_examples, batch_size):\n","        batch_indices = np.array(indices[i:min(i + batch_size, num_examples)])\n","        yield features[batch_indices], labels[batch_indices]"],"outputs":[],"metadata":{"id":"wJuT_xNxctJO","executionInfo":{"status":"ok","timestamp":1630468791750,"user_tz":-480,"elapsed":519,"user":{"displayName":"hello world","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjXbsfhC6hSDTQmFFNhXTddUBf1Yxa-RGB3KxXxFEA=s64","userId":"03112899976397097128"}}}},{"cell_type":"markdown","source":["通常，我们使用合理大小的小批量来利用GPU硬件的优势，因为GPU在并行处理方面表现出色。每个样本都可以并行地进行模型计算，且每个样本损失函数的梯度也可以被并行地计算，GPU可以在处理几百个样本时，所花费的时间不比处理一个样本时多太多。\n","\n","让我们直观感受一下。读取第一个小批量数据样本并打印。每个批量的特征维度说明了批量大小和输入特征数。 同样的，批量的标签形状与`batch_size`相等。"],"metadata":{"id":"7otjj2eTh4w3"}},{"cell_type":"markdown","source":["#### torch版"],"metadata":{"id":"Z0Viv3TKdOFI"}},{"cell_type":"code","execution_count":null,"source":["batch_size = 10\n","\n","for X, y in data_iter(batch_size, features, labels):\n","    print(X, '\\n', y)\n","    break"],"outputs":[],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"IsAdUkTTh9Yt","executionInfo":{"status":"ok","timestamp":1626808748577,"user_tz":-480,"elapsed":7,"user":{"displayName":"hello world","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjXbsfhC6hSDTQmFFNhXTddUBf1Yxa-RGB3KxXxFEA=s64","userId":"03112899976397097128"}},"outputId":"cfa0ef9a-e331-4fd6-f41e-3c8b8f3fb979"}},{"cell_type":"markdown","source":["#### paddle版"],"metadata":{"id":"TQzYIfzVdR_e"}},{"cell_type":"code","execution_count":null,"source":["batch_size = 10\n","\n","for paddle_X, paddle_y in paddle_data_iter(batch_size, paddle_features, paddle_labels):\n","    print(paddle_X, '\\n', paddle_y)\n","    break"],"outputs":[],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"GHiBoKqedUCi","executionInfo":{"status":"ok","timestamp":1626793929585,"user_tz":-480,"elapsed":450,"user":{"displayName":"hello world","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjXbsfhC6hSDTQmFFNhXTddUBf1Yxa-RGB3KxXxFEA=s64","userId":"03112899976397097128"}},"outputId":"b9756cf4-a17e-48e9-8655-bcde8125e362"}},{"cell_type":"markdown","source":["当我们运行迭代时，我们会连续地获得不同的小批量，直至遍历完整个数据集。 上面实现的迭代对于教学来说很好，但它的执行效率很低，可能会在实际问题上陷入麻烦。 例如，它要求我们将所有数据加载到内存中，并执行大量的随机内存访问。 在深度学习框架中实现的内置迭代器效率要高得多，它可以处理存储在文件中的数据和通过数据流提供的数据。"],"metadata":{"id":"RO6NvadSiQu2"}},{"cell_type":"markdown","source":["## 3.2.3 初始化模型参数\n","\n","在开始用小批量随机梯度下降优化模型参数之前，我们需要先有一些参数。 在下面的代码中，我们通过从均值为0、标准差为0.01的正态分布中采样随机数来初始化权重，并将偏置初始化为0。"],"metadata":{"id":"UagpBq97iSmR"}},{"cell_type":"markdown","source":["#### torch版"],"metadata":{"id":"Vg3qZHWOvAAI"}},{"cell_type":"code","execution_count":null,"source":["w = torch.normal(0, 0.01, size=(2, 1), requires_grad=True)\n","b = torch.zeros(1, requires_grad=True)\n","print(w)\n","print(b)"],"outputs":[],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"oq7CzteLighS","executionInfo":{"status":"ok","timestamp":1626793944236,"user_tz":-480,"elapsed":432,"user":{"displayName":"hello world","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjXbsfhC6hSDTQmFFNhXTddUBf1Yxa-RGB3KxXxFEA=s64","userId":"03112899976397097128"}},"outputId":"d25206f8-2e0c-45b4-8e58-79be6fdf1137"}},{"cell_type":"markdown","source":["#### paddle版"],"metadata":{"id":"m3q128WOvM6N"}},{"cell_type":"code","execution_count":null,"source":["# paddle_w = paddle.nn.initializer.Normal(0, 0.01)\n","# paddle_w.stop_gradient = False\n","# paddle_b = paddle.zeros(shape=[1])\n","# paddle_b.stop_gradient = False\n","paddle_w = np.random.normal(0, 0.01, (2, 1))\n","paddle_b = np.zeros(1)\n","print(paddle_w)\n","print(paddle_b)"],"outputs":[],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"BTFkuAu2vPaR","executionInfo":{"status":"ok","timestamp":1626793949724,"user_tz":-480,"elapsed":571,"user":{"displayName":"hello world","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjXbsfhC6hSDTQmFFNhXTddUBf1Yxa-RGB3KxXxFEA=s64","userId":"03112899976397097128"}},"outputId":"ed796fd0-b1a1-4605-a345-8c0e381a8de6"}},{"cell_type":"markdown","source":["在初始化参数之后，我们的任务是更新这些参数，直到这些参数足够拟合我们的数据。 每次更新都需要计算损失函数关于模型参数的梯度。有了这个梯度，我们就可以向减小损失的方向更新每个参数。 因为手动计算梯度很枯燥而且容易出错，所以没有人会手动计算梯度。我们使用 2.5节 中引入的自动微分来计算梯度。"],"metadata":{"id":"t9JoNjWuvKMP"}},{"cell_type":"markdown","source":["## 3.2.4 定义模型\n","接下来，我们必须定义模型，将模型的输入和参数同模型的输出关联起来。 回想一下，要计算线性模型的输出，我们只需计算输入特征  𝐗  和模型权重 𝐰 的矩阵-向量乘法后加上偏置 𝑏 。注意，上面的 𝐗𝐰  是一个向量，而 𝑏 是一个标量。回想一下 2.1.3节 中描述的广播机制。当我们用一个向量加一个标量时，标量会被加到向量的每个分量上。"],"metadata":{"id":"TCIGrTJRzpLm"}},{"cell_type":"markdown","source":["#### torch版"],"metadata":{"id":"PCuMJVswz-8q"}},{"cell_type":"code","execution_count":null,"source":["def linreg(X, w, b):\n","    \"\"\"线性回归模型。\"\"\"\n","    return torch.matmul(X, w) + b"],"outputs":[],"metadata":{"id":"yCtwttBVzy18"}},{"cell_type":"markdown","source":["#### paddle版"],"metadata":{"id":"DDwKbUQ_0EYX"}},{"cell_type":"code","execution_count":null,"source":["def paddle_linreg(X, w, b):\n","    \"\"\"线性回归模型。\"\"\"\n","    # return paddle.matmul(X, w) + b\n","    return np.dot(X, w) + b"],"outputs":[],"metadata":{"id":"xouUYZip0Gfp"}},{"cell_type":"markdown","source":["## 3.2.5 定义损失函数\n","因为要更新模型。需要计算损失函数的梯度，所以我们应该先定义损失函数。 这里我们使用 3.1节 中描述的平方损失函数。 在实现中，我们需要将真实值y的形状转换为和预测值y_hat的形状相同。"],"metadata":{"id":"rbwygpTH0M2Q"}},{"cell_type":"code","execution_count":null,"source":["def squared_loss(y_hat, y):\n","    \"\"\"均方损失。\"\"\"\n","    return (y_hat - y.reshape(y_hat.shape))**2 / 2"],"outputs":[],"metadata":{"id":"XRF40PNI1G2h"}},{"cell_type":"markdown","source":["## 3.2.6 定义优化算法\n","\n","正如我们在 3.1节 中讨论的，线性回归有解析解。然而，这是一本关于深度学习的书，而不是一本关于线性回归的书。 由于这本书介绍的其他模型都没有解析解，下面我们将在这里介绍小批量随机梯度下降的工作示例。\n","\n","在每一步中，使用从数据集中随机抽取的一个小批量，然后根据参数计算损失的梯度。接下来，朝着减少损失的方向更新我们的参数。 下面的函数实现小批量随机梯度下降更新。该函数接受模型参数集合、学习速率和批量大小作为输入。每一步更新的大小由学习速率lr决定。 因为我们计算的损失是一个批量样本的总和，所以我们用批量大小（batch_size）来归一化步长，这样步长大小就不会取决于我们对批量大小的选择。"],"metadata":{"id":"OoDANNV405H7"}},{"cell_type":"markdown","source":["#### torch版"],"metadata":{"id":"tzqyl_G_0b8v"}},{"cell_type":"code","execution_count":null,"source":["def sgd(params, lr, batch_size): \n","    \"\"\"小批量随机梯度下降。\"\"\"\n","    with torch.no_grad():\n","        for param in params:\n","            param -= lr * param.grad / batch_size\n","            param.grad.zero_()"],"outputs":[],"metadata":{"id":"qfKrJnbU0VZs"}},{"cell_type":"markdown","source":["#### paddle版"],"metadata":{"id":"HdxqikUR0fHj"}},{"cell_type":"code","execution_count":null,"source":["def pddle_sgd(params, lr, batch_size):\n","    \"\"\"小批量随机梯度下降。\"\"\"\n","    with paddle.no_grad():\n","        for param in params:\n","            param -= lr * param.grad / batch_size\n","            param.grad.zero_()"],"outputs":[],"metadata":{"id":"Q9ePMWes0mAb"}},{"cell_type":"markdown","source":["## 3.2.7 训练\n","\n","现在我们已经准备好了模型训练所有需要的要素，可以实现主要的训练过程部分了。 理解这段代码至关重要，因为在整个深度学习的职业生涯中，你会一遍又一遍地看到几乎相同的训练过程。 在每次迭代中，我们读取一小批量训练样本，并通过我们的模型来获得一组预测。 计算完损失后，我们开始反向传播，存储每个参数的梯度。最后，我们调用优化算法 sgd 来更新模型参数。\n","\n","接下来，将执行一下循环：\n","\n","- 初始化参数\n","- 重复，直到完成\n","  - 计算梯度 $g \\leftarrow \\partial_{(w,b)}\\frac{1}{\\vert B\\vert}\\sum_{i \\in B}l(x^{(i)},y^{(i)},w,b)$\n","  - 更新参数$(w,b) \\leftarrow (w,b) \\leftarrow \\eta g$ \n","\n","在每个迭代周期（epoch）中，我们使用`data_iter`函数遍历整个数据集，并将训练数据集中所有样本都使用一次（假设样本数能够被批量大小整除）。这里的迭代周期个数`num_epochs`和学习率`lr`都是超参数，分别设为3和0.03。设置超参数很棘手，需要通过反复试验进行调整。 我们现在忽略这些细节，以后会在 2节 中详细介绍。"],"metadata":{"id":"yLl0AWHG0w_H"}},{"cell_type":"markdown","source":["#### torch版"],"metadata":{"id":"wgm8pzuq4Npb"}},{"cell_type":"code","execution_count":null,"source":["lr = 0.03\n","num_epochs = 3\n","net = linreg\n","loss = squared_loss\n","\n","for epoch in range(num_epochs):\n","    for X, y in data_iter(batch_size, features, labels):\n","        l = loss(net(X, w, b), y)  # `X`和`y`的小批量损失\n","        # 因为`l`形状是(`batch_size`, 1)，而不是一个标量。`l`中的所有元素被加到一起，\n","        # 并以此计算关于[`w`, `b`]的梯度\n","        l.sum().backward()\n","        sgd([w, b], lr, batch_size)  # 使用参数的梯度更新参数\n","    with torch.no_grad():\n","        train_l = loss(net(features, w, b), labels)\n","        print(f'epoch {epoch + 1}, loss {float(train_l.mean()):f}')"],"outputs":[],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"rn99IU2S4IlQ","executionInfo":{"status":"ok","timestamp":1626793991046,"user_tz":-480,"elapsed":622,"user":{"displayName":"hello world","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjXbsfhC6hSDTQmFFNhXTddUBf1Yxa-RGB3KxXxFEA=s64","userId":"03112899976397097128"}},"outputId":"50f13095-a247-4de9-e4a7-a8accb13d83b"}},{"cell_type":"markdown","source":["因为我们使用的是自己合成的数据集，所以我们知道真正的参数是什么。 因此，我们可以通过比较真实参数和通过训练学到的参数来评估训练的成功程度。事实上，真实参数和通过训练学到的参数确实非常接近。"],"metadata":{"id":"oeGHiLo24lWl"}},{"cell_type":"code","execution_count":null,"source":["print(f'w的估计误差: {true_w - w.reshape(true_w.shape)}')\n","print(f'b的估计误差: {true_b - b}')"],"outputs":[],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0__yuGJQ4nqF","executionInfo":{"status":"ok","timestamp":1626794003945,"user_tz":-480,"elapsed":774,"user":{"displayName":"hello world","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjXbsfhC6hSDTQmFFNhXTddUBf1Yxa-RGB3KxXxFEA=s64","userId":"03112899976397097128"}},"outputId":"c252b1d4-de1c-41dd-ad32-2123ebda6d67"}},{"cell_type":"markdown","source":["注意，我们不应该想当然地认为我们能够完美地恢复参数。 在机器学习中，我们通常不太关心恢复真正的参数，而更关心那些能高度准确预测的参数。 幸运的是，即使是在复杂的优化问题上，随机梯度下降通常也能找到非常好的解。其中一个原因是，在深度网络中存在许多参数组合能够实现高度精确的预测。"],"metadata":{"id":"0_P8VI624sQ3"}},{"cell_type":"markdown","source":["#### paddle版"],"metadata":{"id":"Ly33OjoP4t2A"}},{"cell_type":"code","execution_count":null,"source":["lr = 0.03\n","num_epochs = 3\n","net = paddle_linreg\n","loss = squared_loss\n","\n","for epoch in range(num_epochs):\n","    for paddle_X, paddle_y in paddle_data_iter(batch_size, features, labels):\n","        l = loss(net(paddle_X, paddle_w, paddle_b), paddle_y)  # `paddle_X`和`paddle_y`的小批量损失\n","        # 因为`l`形状是(`batch_size`, 1)，而不是一个标量。`l`中的所有元素被加到一起，\n","        # 并以此计算关于[`paddle_w`, `paddle_b`]的梯度\n","        l.sum().backward()\n","        paddle_sgd([paddle_w, paddle_b], lr, batch_size)  # 使用参数的梯度更新参数\n","    with paddle.no_grad():\n","        paddle_train_l = loss(net(paddle_features, paddle_w, paddle_b), padle_labels)\n","        print(f'epoch {epoch + 1}, loss {float(pddle_train_l.mean()):f}')"],"outputs":[],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":413},"id":"IMfx3a1q4vnm","executionInfo":{"status":"error","timestamp":1626794256603,"user_tz":-480,"elapsed":586,"user":{"displayName":"hello world","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjXbsfhC6hSDTQmFFNhXTddUBf1Yxa-RGB3KxXxFEA=s64","userId":"03112899976397097128"}},"outputId":"47d49bb9-2422-4082-db66-75a037f43034"}}]}