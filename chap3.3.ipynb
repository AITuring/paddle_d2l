{"nbformat":4,"nbformat_minor":2,"metadata":{"colab":{"name":"chap3.3.ipynb","provenance":[],"toc_visible":true,"authorship_tag":"ABX9TyPrpa9Uk5YHt9EbY0XNNEZ/"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# 3.3 线性回归的简洁实现\n","使用深度学习框架简洁实现上一节中的线性回归模型"],"metadata":{"id":"P2QELrsVU4fK"}},{"cell_type":"code","execution_count":null,"source":["!pip install -U d2l\n","!python -m pip install paddlepaddle -i https://mirror.baidu.com/pypi/simple"],"outputs":[],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"N7pNOlcoUzR7","executionInfo":{"status":"ok","timestamp":1631153660382,"user_tz":-480,"elapsed":29642,"user":{"displayName":"hello world","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjXbsfhC6hSDTQmFFNhXTddUBf1Yxa-RGB3KxXxFEA=s64","userId":"03112899976397097128"}},"outputId":"950aafcd-e3f5-4183-f275-5667f4dc0d64"}},{"cell_type":"code","execution_count":null,"source":["import random\n","import numpy as np\n","# torch\n","import torch\n","from d2l import torch as d2l\n","from torch.utils import data\n","# paddle\n","import paddle\n","import matplotlib.pyplot as plt"],"outputs":[],"metadata":{"id":"73aCqffEVaco","executionInfo":{"status":"ok","timestamp":1631153706265,"user_tz":-480,"elapsed":6159,"user":{"displayName":"hello world","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjXbsfhC6hSDTQmFFNhXTddUBf1Yxa-RGB3KxXxFEA=s64","userId":"03112899976397097128"}}}},{"cell_type":"code","execution_count":null,"source":["def synthetic_data(w, b, num_examples):\n","  \"\"\"Generate y = Xw + b + noise.\"\"\"\n","  X = np.random.normal(0, 1, (num_examples, len(w)))\n","  y = np.dot(X, w) + b\n","  y += np.random.normal(0, 0.01, y.shape)\n","  X = paddle.to_tensor(X)\n","  y = paddle.to_tensor(y)\n","  return X, y.reshape((-1, 1))"],"outputs":[],"metadata":{"id":"IENHcFGcW6mq","executionInfo":{"status":"ok","timestamp":1631189482268,"user_tz":-480,"elapsed":397,"user":{"displayName":"hello world","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjXbsfhC6hSDTQmFFNhXTddUBf1Yxa-RGB3KxXxFEA=s64","userId":"03112899976397097128"}}}},{"cell_type":"markdown","source":["## 3.3.1 生成数据集\n","### torch版"],"metadata":{"id":"CzXiGRhDVlIK"}},{"cell_type":"code","execution_count":null,"source":["torch_w = torch.tensor([2, -3.4])\n","torch_b = 4.2\n","torch_features, torch_labels = d2l.synthetic_data(torch_w, torch_b, 1000)"],"outputs":[],"metadata":{"id":"HoYJcDL6V-MA","executionInfo":{"status":"ok","timestamp":1631181735316,"user_tz":-480,"elapsed":544,"user":{"displayName":"hello world","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjXbsfhC6hSDTQmFFNhXTddUBf1Yxa-RGB3KxXxFEA=s64","userId":"03112899976397097128"}}}},{"cell_type":"code","execution_count":null,"source":["print(torch_features.shape, torch_labels.shape)"],"outputs":[],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"66Fokca7WY7K","executionInfo":{"status":"ok","timestamp":1631153949429,"user_tz":-480,"elapsed":421,"user":{"displayName":"hello world","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjXbsfhC6hSDTQmFFNhXTddUBf1Yxa-RGB3KxXxFEA=s64","userId":"03112899976397097128"}},"outputId":"a2ce375a-b1fe-4803-f0f9-9172d3a349d6"}},{"cell_type":"markdown","source":["### paddle版"],"metadata":{"id":"QWHoEOAMWl2r"}},{"cell_type":"code","execution_count":null,"source":["paddle_w = paddle.to_tensor([2,-3.4])\n","paddle_b = 4.2\n","paddle_features, paddle_labels = synthetic_data(paddle_w, paddle_b, 1000)"],"outputs":[],"metadata":{"id":"OiwpGzVdWowi","executionInfo":{"status":"ok","timestamp":1631154191951,"user_tz":-480,"elapsed":379,"user":{"displayName":"hello world","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjXbsfhC6hSDTQmFFNhXTddUBf1Yxa-RGB3KxXxFEA=s64","userId":"03112899976397097128"}}}},{"cell_type":"code","execution_count":null,"source":["print(paddle_features.shape, paddle_labels.shape)"],"outputs":[],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Uh-b2BPZXcpJ","executionInfo":{"status":"ok","timestamp":1631154216449,"user_tz":-480,"elapsed":432,"user":{"displayName":"hello world","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjXbsfhC6hSDTQmFFNhXTddUBf1Yxa-RGB3KxXxFEA=s64","userId":"03112899976397097128"}},"outputId":"8e730aa5-dfd7-491e-93c8-d5ffde3353e3"}},{"cell_type":"markdown","source":["## 3.3.2 读取数据集\n","可以调用框架中现有的API来读取数据。我们将features和labels作为API的参数传递，并在实例化数据迭代器对象时指定batch_size。此外，布尔值is_train表示是否希望数据迭代器对象在每个迭代周期内打乱数据。\n","\n","### torch版"],"metadata":{"id":"3UoO7qHkZARC"}},{"cell_type":"code","execution_count":null,"source":["def torch_load_array(data_arrays, batch_size, is_train=True):\n","    \"\"\"构造一个PyTorch数据迭代器。\"\"\"\n","    # print(len(data_arrays))\n","    dataset = data.TensorDataset(*data_arrays)\n","    return data.DataLoader(dataset, batch_size, shuffle=is_train)\n","\n","torch_batch_size = 10\n","torch_data_iter = torch_load_array((torch_features, torch_labels), torch_batch_size)"],"outputs":[],"metadata":{"id":"AIFlpQyxZJ5b","executionInfo":{"status":"ok","timestamp":1631181764593,"user_tz":-480,"elapsed":486,"user":{"displayName":"hello world","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjXbsfhC6hSDTQmFFNhXTddUBf1Yxa-RGB3KxXxFEA=s64","userId":"03112899976397097128"}}}},{"cell_type":"markdown","source":["使用data_iter的方式与我们在 3.2节中使用data_iter函数的方式相同。为了验证是否正常工作，让我们读取并打印第一个小批量样本。 与 3.2节不同，这里我们使用iter构造Python迭代器，并使用next从迭代器中获取第一项。"],"metadata":{"id":"rUAAr0rnZk7P"}},{"cell_type":"code","execution_count":null,"source":["# print(type(torch_data_iter))\n","next(iter(torch_data_iter))"],"outputs":[],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"bk_u8vToZmIl","executionInfo":{"status":"ok","timestamp":1631181775987,"user_tz":-480,"elapsed":397,"user":{"displayName":"hello world","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjXbsfhC6hSDTQmFFNhXTddUBf1Yxa-RGB3KxXxFEA=s64","userId":"03112899976397097128"}},"outputId":"45264899-3f24-4a85-a47d-5bffd2d099ca"}},{"cell_type":"markdown","source":["### paddle版"],"metadata":{"id":"RdYECTkHjDa2"}},{"cell_type":"code","execution_count":null,"source":["# 测试TensorDataset\n","from paddle.io import TensorDataset\n","input_np = np.random.random([2, 3, 4]).astype('float32')\n","input = paddle.to_tensor(input_np)\n","label_np = np.random.random([2, 1]).astype('int32')\n","label = paddle.to_tensor(label_np)\n","\n","dataset = TensorDataset([input, label])\n","print(len(dataset))\n","loader = DataLoader(dataset, 10, shuffle=True)\n","\n","def test_load_array(data_arrays, batch_size, is_train=True):\n","    \"\"\"构造一个PyTorch数据迭代器。\"\"\"\n","    print(len(data_arrays))\n","    dataset = TensorDataset(data_arrays)\n","    return DataLoader(dataset, batch_size, shuffle=is_train)\n","\n","test_batch_size = 10\n","test_data_iter = test_load_array((input, label), test_batch_size)"],"outputs":[],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"o2tkkvL7p8ab","executionInfo":{"status":"ok","timestamp":1631179575833,"user_tz":-480,"elapsed":504,"user":{"displayName":"hello world","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjXbsfhC6hSDTQmFFNhXTddUBf1Yxa-RGB3KxXxFEA=s64","userId":"03112899976397097128"}},"outputId":"9f84badb-92e8-4647-c223-992d583d1b99"}},{"cell_type":"code","execution_count":null,"source":["from paddle.io import TensorDataset, DataLoader\n","def paddle_load_array(data_arrays, batch_size, is_train=True):\n","    \"\"\"构造一个paddle数据迭代器。\"\"\"\n","    print(len(data_arrays))\n","    dataset = TensorDataset(data_arrays)\n","    return DataLoader(dataset, batch_size, shuffle=is_train)\n","\n","paddle_batch_size = 10\n","paddle_data_iter = paddle_load_array((paddle_features, paddle_labels), paddle_batch_size)"],"outputs":[],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"NxN1CkZBjFnq","executionInfo":{"status":"ok","timestamp":1631190214380,"user_tz":-480,"elapsed":7,"user":{"displayName":"hello world","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjXbsfhC6hSDTQmFFNhXTddUBf1Yxa-RGB3KxXxFEA=s64","userId":"03112899976397097128"}},"outputId":"9325cea2-621e-4692-91e2-dc6bf3adde61"}},{"cell_type":"code","execution_count":null,"source":["print(type(paddle_data_iter))\n","for X, y in paddle_data_iter:\n","  print(X, y)\n","# next(iter(paddle_data_iter))"],"outputs":[],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":379},"id":"Evhs_ksMr08m","executionInfo":{"status":"error","timestamp":1631190218975,"user_tz":-480,"elapsed":425,"user":{"displayName":"hello world","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjXbsfhC6hSDTQmFFNhXTddUBf1Yxa-RGB3KxXxFEA=s64","userId":"03112899976397097128"}},"outputId":"07c41592-18f1-45aa-df13-dd95cd4435d3"}},{"cell_type":"markdown","source":["## 3.3.3 定义模型\n","当我们在 3.2节中实现线性回归时，我们明确定义了模型参数变量，并编写了计算的代码，这样通过基本的线性代数运算得到输出。但是，如果模型变得更加复杂，而且当你几乎每天都需要实现模型时，你会想简化这个过程。这种情况类似于从头开始编写自己的博客。做一两次是有益的、有启发性的，但如果每次你每需要一个博客就花一个月的时间重新发明轮子，那你将是一个糟糕的网页开发者。\n","\n","对于标准操作，我们可以使用框架的预定义好的层。这使我们只需关注使用哪些层来构造模型，而不必关注层的实现细节。我们首先定义一个模型变量net，它是一个Sequential类的实例。Sequential类为串联在一起的多个层定义了一个容器。当给定输入数据，Sequential实例将数据传入到第一层，然后将第一层的输出作为第二层的输入，依此类推。在下面的例子中，我们的模型只包含一个层，因此实际上不需要Sequential。但是由于以后几乎所有的模型都是多层的，在这里使用Sequential会让你熟悉标准的流水线。\n","\n","回顾 图3.1.2中的单层网络架构，这一单层被称为全连接层（fully-connected layer），因为它的每一个输入都通过矩阵-向量乘法连接到它的每个输出。\n","\n","### torch版\n","\n","在PyTorch中，全连接层在Linear类中定义。值得注意的是，我们将两个参数传递到nn.Linear中。第一个指定输入特征形状，即2，第二个指定输出特征形状，输出特征形状为单个标量，因此为1。"],"metadata":{"id":"cWMMUEn1fgt0"}},{"cell_type":"code","execution_count":null,"source":["# `nn` 是神经网络的缩写\n","from torch import nn\n","\n","torch_net = nn.Sequential(nn.Linear(2, 1))"],"outputs":[],"metadata":{"id":"MrFQpnlOfwcl","executionInfo":{"status":"ok","timestamp":1631181807806,"user_tz":-480,"elapsed":420,"user":{"displayName":"hello world","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjXbsfhC6hSDTQmFFNhXTddUBf1Yxa-RGB3KxXxFEA=s64","userId":"03112899976397097128"}}}},{"cell_type":"markdown","source":["### paddle版"],"metadata":{"id":"7X9DKwHCf-dR"}},{"cell_type":"code","execution_count":null,"source":["paddle_net = paddle.nn.Sequential(paddle.nn.Linear(in_features=2, out_features=1))\n","print(paddle_net)"],"outputs":[],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_Fl6dnvigAqN","executionInfo":{"status":"ok","timestamp":1631179942705,"user_tz":-480,"elapsed":482,"user":{"displayName":"hello world","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjXbsfhC6hSDTQmFFNhXTddUBf1Yxa-RGB3KxXxFEA=s64","userId":"03112899976397097128"}},"outputId":"5ef0288f-88be-466b-aa0e-ae02ba3efb9e"}},{"cell_type":"markdown","source":["## 3.3.4 初始化模型参数\n","在使用net之前，我们需要初始化模型参数。如在线性回归模型中的权重和偏置。 深度学习框架通常有预定义的方法来初始化参数。 在这里，我们指定每个权重参数应该从均值为0、标准差为0.01的正态分布中随机采样，偏置参数将初始化为零。\n","\n","### torch版\n","\n","正如我们在构造nn.Linear时指定输入和输出尺寸一样。现在我们直接访问参数以设定初始值。我们通过net[0]选择网络中的第一个图层，然后使用weight.data和bias.data方法访问参数。然后使用替换方法normal_和fill_来重写参数值。"],"metadata":{"id":"yvaA9mR1gG1M"}},{"cell_type":"code","execution_count":null,"source":["torch_net[0].weight.data.normal_(0, 0.01)\n","torch_net[0].bias.data.fill_(0)"],"outputs":[],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"xgdIIb1whC73","executionInfo":{"status":"ok","timestamp":1631181815792,"user_tz":-480,"elapsed":502,"user":{"displayName":"hello world","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjXbsfhC6hSDTQmFFNhXTddUBf1Yxa-RGB3KxXxFEA=s64","userId":"03112899976397097128"}},"outputId":"593bb187-7b78-4e7e-a254-90cdf73bb574"}},{"cell_type":"markdown","source":["### paddle版"],"metadata":{"id":"NVYYt_6E5GMn"}},{"cell_type":"code","execution_count":null,"source":["# 先放着\n","# paddle_net[0].weight.data.normal_(0, 0.01)\n","# paddle_net[0].weight.data\n","# paddle_net[0].bias.data.fill_(0)"],"outputs":[],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":202},"id":"JVUoi4CJ5JZx","executionInfo":{"status":"error","timestamp":1631180426034,"user_tz":-480,"elapsed":509,"user":{"displayName":"hello world","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjXbsfhC6hSDTQmFFNhXTddUBf1Yxa-RGB3KxXxFEA=s64","userId":"03112899976397097128"}},"outputId":"7e3b060d-d126-471f-d178-7bdcf08997e6"}},{"cell_type":"markdown","source":["## 3.3.5 定义损失函数\n","计算均方误差使用的是MSELoss类，也称为平方 𝐿2\n"," 范数。默认情况下，它返回所有样本损失的平均值。\n","\n","### torch版"],"metadata":{"id":"JOiQSTGR9PKi"}},{"cell_type":"code","execution_count":null,"source":["torch_loss = nn.MSELoss()"],"outputs":[],"metadata":{"id":"iNINOYJu9ZEj","executionInfo":{"status":"ok","timestamp":1631181824241,"user_tz":-480,"elapsed":408,"user":{"displayName":"hello world","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjXbsfhC6hSDTQmFFNhXTddUBf1Yxa-RGB3KxXxFEA=s64","userId":"03112899976397097128"}}}},{"cell_type":"markdown","source":["### paddle版"],"metadata":{"id":"cNFp8kVV9dWZ"}},{"cell_type":"code","execution_count":null,"source":["paddle_loss = paddle.nn.MSELoss()"],"outputs":[],"metadata":{"id":"XEpJcI_l9e2d","executionInfo":{"status":"ok","timestamp":1631180970487,"user_tz":-480,"elapsed":410,"user":{"displayName":"hello world","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjXbsfhC6hSDTQmFFNhXTddUBf1Yxa-RGB3KxXxFEA=s64","userId":"03112899976397097128"}}}},{"cell_type":"markdown","source":["## 3.3.6 定义优化算法\n","\n","小批量随机梯度下降算法是一种优化神经网络的标准工具，PyTorch在optim模块中实现了该算法的许多变种。当我们实例化SGD实例时，我们要指定优化的参数（可通过net.parameters()从我们的模型中获得）以及优化算法所需的超参数字典。小批量随机梯度下降只需要设置lr值，这里设置为0.03。\n","\n","### torch版"],"metadata":{"id":"Ki0yB9S-9mUi"}},{"cell_type":"code","execution_count":null,"source":["torch_trainer = torch.optim.SGD(torch_net.parameters(), lr=0.03)"],"outputs":[],"metadata":{"id":"sCIf10zg9uuJ","executionInfo":{"status":"ok","timestamp":1631181831205,"user_tz":-480,"elapsed":631,"user":{"displayName":"hello world","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjXbsfhC6hSDTQmFFNhXTddUBf1Yxa-RGB3KxXxFEA=s64","userId":"03112899976397097128"}}}},{"cell_type":"markdown","source":["### paddle版"],"metadata":{"id":"673nS2P09-mY"}},{"cell_type":"code","execution_count":null,"source":["paddle_trainer = paddle.optimizer.SGD(learning_rate=0.03, parameters=paddle_net.parameters())"],"outputs":[],"metadata":{"id":"2FLUnonv-AOz","executionInfo":{"status":"ok","timestamp":1631181155304,"user_tz":-480,"elapsed":396,"user":{"displayName":"hello world","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjXbsfhC6hSDTQmFFNhXTddUBf1Yxa-RGB3KxXxFEA=s64","userId":"03112899976397097128"}}}},{"cell_type":"markdown","source":["## 3.3.7 训练\n","\n","通过深度学习框架的高级API来实现我们的模型只需要相对较少的代码。 我们不必单独分配参数、不必定义我们的损失函数，也不必手动实现小批量随机梯度下降。 当我们需要更复杂的模型时，高级API的优势将大大增加。 当我们有了所有的基本组件，训练过程代码与我们从零开始实现时所做的非常相似。\n","\n","回顾一下：在每个迭代周期里，我们将完整遍历一次数据集（train_data），不停地从中获取一个小批量的输入和相应的标签。对于每一个小批量，我们会进行以下步骤:\n","- 通过调用net(X)生成预测并计算损失l（正向传播）。\n","- 通过进行反向传播来计算梯度。\n","- 通过调用优化器来更新模型参数。\n","\n","为了更好的衡量训练效果，我们计算每个迭代周期后的损失，并打印它来监控训练过程。\n","\n","### torch版\n"],"metadata":{"id":"A1wGQyLz-UNk"}},{"cell_type":"code","execution_count":null,"source":["num_epochs = 3\n","for epoch in range(num_epochs):\n","    for X, y in torch_data_iter:\n","        l = torch_loss(torch_net(X) ,y)\n","        torch_trainer.zero_grad()\n","        l.backward()\n","        torch_trainer.step()\n","    l = torch_loss(torch_net(torch_features), torch_labels)\n","    print(f'epoch {epoch + 1}, loss {l:f}')"],"outputs":[],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ujSvWo0c-gV7","executionInfo":{"status":"ok","timestamp":1631181865132,"user_tz":-480,"elapsed":393,"user":{"displayName":"hello world","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjXbsfhC6hSDTQmFFNhXTddUBf1Yxa-RGB3KxXxFEA=s64","userId":"03112899976397097128"}},"outputId":"45dcf327-51f2-4b98-c3b7-bc6da93b56fc"}},{"cell_type":"markdown","source":["下面我们比较生成数据集的真实参数和通过有限数据训练获得的模型参数。 要访问参数，我们首先从net访问所需的层，然后读取该层的权重和偏置。 正如在从零开始实现中一样，我们估计得到的参数与生成数据的真实参数非常接近。"],"metadata":{"id":"s-7baFLDALhV"}},{"cell_type":"code","execution_count":null,"source":["w = torch_net[0].weight.data\n","print('w的估计误差：', torch_w - w.reshape(torch_w.shape))\n","b = net[0].bias.data\n","print('b的估计误差：', torch_b - b)"],"outputs":[],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"CODHU3e3AMdl","executionInfo":{"status":"ok","timestamp":1631181893907,"user_tz":-480,"elapsed":376,"user":{"displayName":"hello world","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjXbsfhC6hSDTQmFFNhXTddUBf1Yxa-RGB3KxXxFEA=s64","userId":"03112899976397097128"}},"outputId":"6394bee1-44cb-4361-b2e7-3d450d2e2c56"}},{"cell_type":"markdown","source":["### paddle版"],"metadata":{"id":"4xT9rxW2BM63"}},{"cell_type":"code","execution_count":null,"source":["num_epochs = 3\n","for epoch in range(num_epochs):\n","    for X, y in paddle_data_iter:\n","        l = paddle_loss(paddle_net(X) ,y)\n","        paddle_trainer.zero_grad()\n","        l.backward()\n","        paddle_trainer.step()\n","    l = paddle_loss(paddle_net(paddle_features), paddle_labels)\n","    print(f'epoch {epoch + 1}, loss {l:f}')\n","# for epoch in range(num_epochs):\n","#   y_predict = paddle_net(paddle_features)\n","#   loss=paddle_loss(y_predict, paddle_labels)\n","#   loss.backward()\n","#   paddle_trainer .step()\n","#   paddle_trainer .clear_grad()\n","#   print(f'epoch {epoch + 1}, loss {loss:f}')"],"outputs":[],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":379},"id":"HzEQBBwUBP-r","executionInfo":{"status":"error","timestamp":1631190187329,"user_tz":-480,"elapsed":426,"user":{"displayName":"hello world","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjXbsfhC6hSDTQmFFNhXTddUBf1Yxa-RGB3KxXxFEA=s64","userId":"03112899976397097128"}},"outputId":"b90d5f46-9ace-4857-8d16-b3e8e712e984"}}]}